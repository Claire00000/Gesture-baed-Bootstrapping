import numpy as np
import tables as tb
from scipy.sparse import csr_matrix
import h5py
import sys
caffe_root = '/home/yuboz/caffe_ts/caffe'  # this file should be run from {caffe_root}/examples (otherwise change this line)
sys.path.insert(0, caffe_root + '/python')
import caffe

class EuclideanLossLayer(caffe.Layer):
    """
    Compute the Euclidean Loss in the same manner as the C++ EuclideanLossLayer
    to demonstrate the class interface for developing layers in Python.
    """

    def setup(self, bottom, top):
        # check input pair
        if len(bottom) != 3:
            raise Exception("Need three inputs to compute distance.")

    def reshape(self, bottom, top):
        # check input dimensions match
        #print bottom[0].count 
        #tmp = bottom[0].data
        #tmp1 = tmp[:,1,:,:]
        #bottom[0].reshape(1,1,380,1030)
        #bottom[0].data[...] = tmp1
        if bottom[0].count != bottom[1].count:
            raise Exception("Inputs must have the same dimension.")
        if bottom[0].count != bottom[2].count:
            raise Exception("Inputs and weights must have the same dimension.")
        # difference is shape of inputs
        self.sig = np.zeros_like(bottom[0].data, dtype=np.float64)
        self.diff = np.zeros_like(bottom[1].data, dtype=np.float64)
        self.mask = np.zeros_like(bottom[1].data, dtype=np.float64)
        #self.w = np.zeros_like(bottom[1].data, dtype=np.float64)
        self.tran = np.zeros_like(bottom[0].data, dtype=np.int8)
        self.seed = np.zeros_like(bottom[0].data, dtype=np.float32)
        self.mask[...] = bottom[1].data
        self.mask[self.mask==1] = 4
        self.mask[self.mask==0] = 0.6
        # nor = bottom[2].data
        # a = nor[0,0,:,:]
        # var_max = np.amax(a)
        # var_min = np.amin(a)
        # self.w[0,0,:,:] = ((a-var_min)/(var_max-var_min))/np.sum((a-var_min)/(var_max-var_min))*380*1030
        # for i in range(380):
        #     for j in range(1030):
        #         print self.w[0,0,i,j]
        # loss output is scalar
        top[0].reshape(1)

    def forward(self, bottom, top):
        #num = bottom[2].data
        #path = '/home/yuboz/caffe1/caffe/result4/output_image1/'+ str(num)+'.h5'
        #h5 = tb.open_file(path, 'r')
        #a = h5.root.data
        self.sig[...] = 1/(1+np.exp(-1*0.3*bottom[0].data))
        self.diff[...] = self.sig[...] - bottom[1].data
	a = bottom[2].data
        #for i in range(380):
            #for j in range(1030):
                
		#print a[0,0,i,j]
        #mid = np.zeros((1,1030*380))
        #for i in range(1030*380):
        #    mid[i] = np.dot(self.diff,a[:,i])
        #for 

        
        top[0].data[...] = np.sum(self.diff**2 * self.mask * bottom[2].data) / bottom[0].num / 2.

    def backward(self, top, propagate_down, bottom):
        #self.seed[...] = (np.random.rand(380,1030)-0.5)*2
        #self.tran[self.seed>0.7] = 3
        #self.tran[self.seed<-0.7] = -3

        for i in range(2):
            if not propagate_down[i]:
                continue
            if i == 0:
                sign = 1
            else:
                sign = -1
            bottom[i].diff[...] = sign * 0.3*self.sig[...]*(1-self.sig[...])*self.diff *bottom[2].data* self.mask / bottom[i].num
        # d= bottom[0].data
        # g = bottom[1].data
        # out = 'output is  '+ str(d[0,0,250,100])
        # gt = 'gt is   '+ str(g[0,0,250,100])
        # grad = 'gradient back is    '+ str(bottom[0].diff[0,0,250,100])
        # diff = 'the sigmoid is    '+ str(self.sig[0,0,250,100])
        # m = 'mask is    '+str(self.mask[0,0,250,100])
        # print d
        # print gt
        # print grad
        # print diff
        # print m
        # print 'second: 0 part'
        # out = 'output is  '+ str(d[0,0,250,500])
        # gt = 'gt is   '+ str(g[0,0,250,500])
        # grad = 'gradient back is    '+ str(bottom[0].diff[0,0,250,500])
        # diff = 'the sigmoid is    '+ str(self.sig[0,0,250,500])
        # m = 'mask is    '+str(self.mask[0,0,250,500])
        # print out
        # print gt
        # print grad
        # print diff
        # print m
            #print np.amax(np.absolute(bottom[i].diff[...]))
            #print np.sum(np.sum(np.sum(np.sum(bottom[1].data,axis= 0),axis=0),axis= 0),axis=0)

